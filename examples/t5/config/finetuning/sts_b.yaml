# @package _group_

common:
  fp16: true
  fp16_init_scale: 4
  threshold_loss_scale: 1
  fp16_scale_window: 128
  log_format: json
  log_interval: 1000000
  seed: ???

task:
  _name: t5_sentence_prediction
  data: ???
  shorten_method: truncate
  tokens_per_sample: 512
  init_token: 0
  separator_token: 2
  prefix_0: "stsb sentence1:"
  prefix_1: "sentence2:"
  regression_target: true
  num_classes: 1
  regression_quant_mul: 20
  regression_quant_frac: 2
  regression_fallback: 0.5
  eval_generate: true
  eval_generate_args: '{"beam": 1, "max_len_b": 20}'
  eval_generate_print_samples: false

checkpoint:
  restore_file: ???
  reset_optimizer: true
  reset_dataloader: true
  reset_meters: true
  best_checkpoint_metric: pearson_spearman
  maximize_best_checkpoint_metric: true
  no_epoch_checkpoints: true
  no_save: ???

distributed_training:
  find_unused_parameters: true
  distributed_world_size: 1

criterion:
  _name: label_smoothed_cross_entropy

dataset:
  batch_size: ???
  required_batch_size_multiple: 1

optimizer:
  _name: adam
  weight_decay: 0.1
  adam_betas: (0.9,0.98)
  adam_eps: 1e-06

lr_scheduler:
  _name: polynomial_decay
  warmup_updates: ???

optimization:
  clip_norm: 0.0
  lr: ???
  max_update: ???
  max_epoch: ???

model:
  _name: transformer_t5_base
  max_positions: 512
